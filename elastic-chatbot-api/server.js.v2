require("dotenv").config();
const express = require("express");
const cors = require("cors");
const { Client } = require("@elastic/elasticsearch");
const axios = require("axios");

const app = express();
const PORT = 3000;

app.use(cors());
app.use(express.json());

const esClient = new Client({ node: "http://localhost:9200" });

// Route to handle chatbot requests
app.post("/chat", async (req, res) => {
  const userMessage = req.body.message;

  // Search Elasticsearch for a relevant answer
  const { hits } = await esClient.search({
    index: "chatbot",
    query: { match: { question: userMessage } },
  });

  if (hits.hits.length > 0) {
    return res.json({ response: hits.hits[0]._source.answer });
  }

  // If no answer is found, fallback to LM Studio API
  try {
    const lmStudioResponse = await axios.post(
      "http://localhost:1234/v1/chat/completions",
      {
        model: "dolphin3.0-llama3.1-8b",  // Replace with your model's name from /v1/models
        messages: [{ role: "user", content: userMessage }],
      }
    );

    return res.json({ response: lmStudioResponse.data.choices[0].message.content });
  } catch (error) {
    return res.status(500).json({ response: "Error fetching response from LM Studio" });
  }
});

// Start the backend server
app.listen(PORT, () => console.log(`Backend running on http://localhost:${PORT}`));
